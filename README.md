# RNN Attention

Репозиторий содержит эксперимент по обучению Seq2Seq модели с разными видами механизма attention. 
Для эксперимента реализованы Bahdanau attention и Luong Attention, Luong Attention обобщаются способы: 

- dot attention: $s_i^Th_j$ — скалярно перемножаем скрытые слои энкодера и декодера
- general attention: $s_i^TW_1h_j$ —  перемножаем скрытые слои энкодера и декодера с промежуточной матрицей весов между ними
- concat attention: $v^Ttanh(W_1[s_i;h_j])$ — умножаем обучаемый вектор весов на тангенс от взвешенного векторного произведения скрытых слоёв энкодера и декодера (почти Bahdanau)

Bahdanau attention: $e_{ij} = tanh(h_jW_1 + s_iW_2)v$, где $W_1, W_2$ - обучаемые матрицы весов, а $v$ - обучаемый вектор весов.

Реализованные подходы к вниманию вы можете найти в файле `attentions.py`, а сами модели в файле `models.py`.

## Результаты
![](results/All_results.jpg)
![](results/Losses.png)

